
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<title>Wei Jiang @ LAMDA, NJU-CS</title>
	<style>
		h2 {
			color: #880000;
			font-weight: 600;
		}
	</style>
</head>

<body>
	<div id="layout-content">
		<div id="toptitle">
			<h1>Wei Jiang @ LAMDA, NJU-CS</h1>
		</div>
		<table class="imgtable">
			<tbody>
				<tr>
					<td>
						<a href="./index_files/jiangwei.jpg"><img src="./index_files/jiangwei.jpg" alt="jiangwei.jpg"
								width="197px" height="202px"></a>&nbsp;</td>
					<td align="left">
						<p><b>
								<font size="+4" face="STKaiti">姜伟</font>
							</b><br>
							<font size="+2">Wei Jiang</font>
							<br>
							Ph.D. student, <a href="http://www.lamda.nju.edu.cn/CH.MainPage.ashx">LAMDA Group</a><br>
							<a href="https://cs.nju.edu.cn/">Department of Computer Science and Technology</a> <br>
							<a href="https://keysoftlab.nju.edu.cn/">National Key Laboratory for Novel Software
								Technology</a> <br>
							<a href="https://www.nju.edu.cn/">Nanjing University</a>, Nanjing 210023, China<br><br>
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=TxdsiyYAAAAJ&view_op=list_works">Google Scholar</a>

					</td>
					<td valign="top" width="179">
						<p align="center"><a href="http://www.lamda.nju.edu.cn/" target="_blank"><img height="85"
									src="./index_files/lamda.jpg" width="179" border="0" align="top"></a></p>
						<p align="center"><a href="http://www.nju.edu.cn/" target="_blank"><img height="120"
									src="./index_files/nju.jpg" width="100" border="0"></a></p>
					</td>
				</tr>
			</tbody>
		</table>
		</td>
		</tr>
		</tbody>
		</table><br>


		<h2>Supervisor</h2>
		<p> &nbsp; &nbsp;&nbsp;&nbsp; Professor <a href="http://ai.nju.edu.cn/zlj/" target="_blank"> Lijun Zhang </a> <br></p>

		<h2>Education Experiences</h2>
		<ul>
			<li>
				<p><b>2020.09 - Now</b>, Ph.D., Computer Science and Technology, <a href="http://www.nju.edu.cn/"
						target="_blank">Nanjing University</a>.
				<ul>
	   			<li> 2023.10 - 2024.05, Visiting Student, <a href="https://nus.edu.sg/"
						target="_blank">National University of Singapore</a>.
				</ul>
					</p>
			</li>
			<li>
				<p><b>2016.09 - 2020.06</b>, B.E., Computer Science and Technology (Honors Science Program), <a href="http://www.xjtu.edu.cn/">Xi'an Jiaotong University</a>.
					<ul>
	   			<li> 2019.01 - 2019.06, Exchange Student, <a href="https://www.berkeley.edu/"
						target="_blank">University of California, Berkeley.</a>  [<a href="./pdf/UCB.pdf" target="_blank">certificate</a>][<a href="./pdf/Transcript.pdf" target="_blank">transcript</a>]</li>
				</ul>
				<ul>
	   			<li> 2018.07 - 2018.08, Exchange Student, <a href="https://www.manchester.ac.uk/"
						target="_blank">University of Manchester.</a>  [<a href="./pdf/Manchester.pdf" target="_blank">certificate</a>]</li>
				</ul>
			</li>
		</ul>

		<h2>Research Interests</h2>
		<p>&nbsp; &nbsp;&nbsp;&nbsp; My research interests include <b>Machine Learning</b>, <b>Stochastic Optimization</b> and <b>Online learning</b>.</p> 

		<h2>Journal</h2>
		<ol>
				<li> Revisiting Stochastic Multi-Level Compositional Optimization. [<a href="./pdf/TPAMI-2025-Jiang.pdf" target="_blank">PDF</a>]  <br>   
						<b>W. Jiang</b>, S. Yang, Y. Wang, T. Yang, and L. Zhang<br>
						IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>),  in press, 2025.<br><br>
				<li> Normalized Adaptive Variance Reduction Method.   <br>   
						<b>W. Jiang</b>, S. Yang, Y. Wang, and L. Zhang<br>
						Journal of Software,  to appear, 2025.<br><br>
		</ol>				
		<h2>Conference</h2>
		<ol>

				<li> Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions  [<a href="./pdf/NeurIPS-2024-Jiang-A.pdf" target="_blank">PDF</a>]<br>   
						<b>W. Jiang</b>, S. Yang, Y. Wang, and L. Zhang<br>
						In Advances in Neural Information Processing Systems 37 (<b>NeurIPS 2024</b>),  pages 22047 - 22080, 2024.<br><br>

				<li> Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction  [<a href="./pdf/NeurIPS-2024-Jiang-B.pdf" target="_blank">PDF</a>]<br>    
                    <b>W. Jiang</b>, S. Yang, W. Yang, and L. Zhang <br>
                    In Advances in Neural Information Processing Systems 37 (<b>NeurIPS 2024</b>),  pages 33891 - 33932, 2024.<br><br>

	
				<li> Online Composite Optimization Between Stochastic and Adversarial Environments [<a href="./pdf/NeurIPS-2024-Wang.pdf" target="_blank">PDF</a>]<br>  
						Y. Wang, S. Chen, <b>W. Jiang</b>, W. Yang,  Y.Wan and L. Zhang <br>
						In Advances in Neural Information Processing Systems 37 (<b>NeurIPS 2024</b>),   pages 94808 - 94850, 2024.<br><br>		
				
				<li> Projection-Free Variance Reduction Methods for Stochastic Constrained Multi-Level Compositional Optimization [<a href="./pdf/ICML-2024-Jiang.pdf" target="_blank">PDF</a>]<br>           
				   <b>W. Jiang</b>, S. Yang, W. Yang, Y. Wang, Y. Wan, and L. Zhang<br> 
					 In Proceedings of the 41st International Conference on Machine Learning (<b>ICML 2024</b>), pages 21962 - 21987, 2024. <br><br>

				<li> Small-loss Adaptive Regret for Online Convex Optimization [<a href="./pdf/ICML-2024-Yang.pdf" target="_blank">PDF</a>]<br> 
					 W. Yang, <b>W. Jiang</b>, Y. Wang, P. Yang, Y. Hu, and L. Zhang<br> 
					 In Proceedings of the 41st International Conference on Machine Learning (<b>ICML 2024</b>), pages 56156 - 56195, 2024. <br><br>

				<li> Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond [<a href="./pdf/ICML-2024-Yu.pdf" target="_blank">PDF</a>]<br>      
				    D. Yu, Y. Cai, <b>W. Jiang</b>, and L. Zhang<br>
					 In Proceedings of the 41st International Conference on Machine Learning (<b>ICML 2024</b>), pages 57384 - 57414, 2024. <br><br> 

				<li> Non-stationary Projection-free Online Learning with Dynamic and Adaptive Regret Guarantees [<a href="./pdf/AAAI-2024-Wang.pdf" target="_blank">PDF</a>, <a href="https://arxiv.org/abs/2305.11726" target="_blank">arXiv</a>]<br>   
					Y. Wang, W. Yang, <b>W. Jiang</b>, S. Lu, B. Wang, H. Tang, Y. Wan, and L. Zhang<br>	
					In Proceedings of the 38th AAAI Conference on Artificial Intelligence (<b>AAAI 2024</b>),  pages 15671 - 15679, 2024. <br><br>

				

				<li> Learning Unnormalized Statistical Models via Compositional Optimization  [<a href="./pdf/ICML-2023-Jiang.pdf" target="_blank">PDF</a>]<br>   
				<b>W. Jiang</b>, J. Qin, L. Wu, C. Chen, T. Yang, L. Zhang<br>	
				In Proceedings of the 40th International Conference on Machine Learning (<b>ICML 2023</b>), pages 15105 - 15124, 2023.<br><br>
				<li> Multi-block-Single-probe Variance Reduced Estimator for Coupled Compositional Optimization [<a href="./pdf/NeurIPS-2022-Jiang.pdf" target="_blank">PDF</a>, <a href="./pdf/NeurIPS-2022-Jiang-S.pdf" target="_blank">Supplementary</a>] <br>   
				<b>W. Jiang</b>, G. Li, Y. Wang, L. Zhang, and T. Yang<br>
				In Advances in Neural Information Processing Systems 35 (<b>NeurIPS 2022</b>), pages 32499 - 32511, 2022. <br><br>
				<li> Smoothed Online Convex Optimization Based on Discounted-Normal-Predictor  [<a href="./pdf/NeurIPS-2022-Zhang.pdf" target="_blank">PDF</a>, <a href="./pdf/NeurIPS-2022-Zhang-S.pdf" target="_blank">Supplementary</a>]<br>   
				L. Zhang, <b>W. Jiang</b>, J. Yi, and T. Yang<br>
				In Advances in Neural Information Processing Systems 35 (<b>NeurIPS 2022</b>), pages 4928 - 4942, 2022. <br><br>
				<li> Optimal Algorithms for Stochastic Multi-Level Compositional Optimization  [<a href="./pdf/ICML-2022-Jiang.pdf" target="_blank">PDF</a>]<br>   
				<b>W. Jiang</b>, B. Wang, Y. Wang, L. Zhang, and T. Yang<br>	
				In Proceedings of the 39th International Conference on Machine Learning (<b>ICML 2022</b>), pages 10195 - 10216, 2022.<br><br>
                <li> Revisiting Smoothed Online Learning  [<a href="./pdf/NeurIPS-2021-Zhang-A.pdf" target="_blank">PDF</a>, <a href="./pdf/NeurIPS-2021-Zhang-A-S.pdf" target="_blank">Supplementary</a>]<br>
	L. Zhang, <b>W. Jiang</b>, S. Lu, and T. Yang<br>
	In Advances in Neural Information Processing Systems 34 (<b>NeurIPS 2021</b>), pages 13599 - 13612, 2021. <br><br>
                <li> Dual Adaptivity: A Universal Algorithm for Minimizing the Adaptive Regret of Convex Functions [<a href="./pdf/NeurIPS-2021-Zhang-B.pdf" target="_blank">PDF</a>, <a href="./pdf/NeurIPS-2021-Zhang-B-S.pdf" target="_blank">Supplementary</a>]<br>  
				L. Zhang, G. Wang, W.-W. Tu,  <b>W. Jiang</b>, and Z.-H. Zhou<br>
				In Advances in Neural Information Processing Systems 34 (<b>NeurIPS 2021</b>), pages 24968 - 24980, 2021. <br><br>
            </ol>

		<h2>Honors and Awards</h2>
            <ul>
            	<li>National Scholarship, 2024 </li>
            	<li>Excellent Student of Nanjing University, 2024</li>
            	<li>NeurIPS Top Reviewer, 2024
            	<li>National Scholarship, 2023 </li>
            	<li>Excellent Student of Nanjing University, 2023</li>
            	<li>LAMDA Elite Award, 2023</li>
            	<li>Tencent Scholarship, 2022</li>
            	<li>Excellent Student of Nanjing University, 2022</li>
            	<li>Industrial Bank Scholarship, 2021</li>
                <li>Excellent Student of Nanjing University, 2021</li>
                <li>Grand Champion of DeeCamp Artificial Intelligence Camp, 2020 (￥100,000)</li>
                <li>Excellent Graduate of Xi'an jiaotong University, 2020</li>
            </ul>

        <h2>Foundation</h2>
		<p><b>Distributed Optimization of Compositional Loss Functions.</b> 
		Postgraduate Research & Practice Innovation Program of Jiangsu Province (KYCX24_0231) 2024.05-2025.05</p>

		<h2>Academic Service</h2>
            <ul>
            	<li>Area Chair:  NeurIPS 2025.</li>
            	<li>Reviewer for Conferences:  ICML 2025,2024,2023,2022; NeurIPS 2024,2023,2022; ICLR 2025,2024; AAAI 2025; AISTATS 2023.</li>
                <li>Reviewer for Journal: IEEE Transactions on Information Forensics & Security; IEEE Transactions on Evolutionary Computation; Machine Learning; Information Sciences; Neurocomputing; TMLR.</li>
                
            </ul>

			<h2>Teaching Assistant</h2>
            <ul>
            	<li><a href="http://www.lamda.nju.edu.cn/zhuangzh/cm2023.html" target="_blank">Computational Methods</a> (With Prof. <a href="http://ai.nju.edu.cn/zlj/" target="_blank">Lijun Zhang</a>; For Undergraduate Student, Spring, 2023)</li>
            	<li><a href="http://www.lamda.nju.edu.cn/qiuzh/optfall2021gra.html" target="_blank">Optimization Methods</a> (With Prof. <a href="http://ai.nju.edu.cn/zlj/" target="_blank">Lijun Zhang</a>; For Graduate Student, Fall, 2021)</li>
                <li><a href="http://www.lamda.nju.edu.cn/chengq/course/opt2020/opt2020.html" target="_blank">Optimization Methods</a> (With Prof. <a href="http://ai.nju.edu.cn/zlj/" target="_blank">Lijun Zhang</a>; For Undergraduate Student, Fall, 2020)</li>
                
            </ul>


		<h2>Correspondence</h2>
		<ul>
			<li>
				<p>Email: <a href="mailto:jiangw@lamda.nju.edu.cn">
						jiangw@lamda.nju.edu.cn
					</a></p>
			</li>
			<li>
				<p>Office:
					Room 912, Computer Science Building, Xianlin Campus of Nanjing University<br>
					</p>
			</li>
		</ul>

		



		<div id="footer">
			<div id="footer-text">
				Last Modified: 2025.04.25
			</div>
		</div>
	</div>


	<style>
		@font-face {
			font-family: "yourDictFontAwesome";
			src: url("chrome-extension://dmckmhkomggmpalekfadjibdcknieljf/lib/fontawesome-webfont.ttf") format("truetype");
			font-weight: normal;
			font-style: normal;
		}
	</style>
</body>

</html>